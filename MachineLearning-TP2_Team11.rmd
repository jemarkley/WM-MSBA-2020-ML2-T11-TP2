---
title: "MSBA ML2 TP2 Team 11"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Team 11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load packages and read files
```{r load packages}
library(tidyverse)
library(data.table)
library(lubridate)
library(magrittr)
library(Matrix)
library(xgboost)
library(parallel)
library(pls)
library(glmnet)
library(splines)
library(gam)
set.seed(5072)
```

Read file
```{r read file}
sales_train <- read.csv("sales_train.csv") # the training set. Daily historical data from January 2013 to October 2015.
test_input <- read.csv("test.csv")
item_categories <- read.csv("item_categories.csv") # supplemental information about the items categories.
items <- read.csv("items.csv") # supplemental information about the items/products.
shops <- read.csv("shops.csv") # supplemental information about the shops.
```
1. Merge Dataset
1.1 Merge train dataset
```{r merge train data}
train <- sales_train %>% 
  left_join(items,by = "item_id")

train$item_name <- NULL
```

1.2 Merge test 
```{r merge test data}
test <- test_input %>% 
  left_join(items, by = "item_id") 

test$item_name <- NULL
test$ID <- NULL
test$date_block_num <- 34
```

2. Check data quality
2.1 Check data quality - missing value
```{r missing value}
colSums(is.na(train))
colSums(is.na(test))
```
There are no missing value for both training and testing datasets

2.2 Check data quality - negative value
```{r negative}
negative <- filter(train, item_cnt_day < 0 | item_price <= 0)
count(negative)
train <- filter(train, item_cnt_day >= 0 & item_price > 0)
dim(train)
```
item_cnt_day for some rows is "-1", which I don't know wheather it's caused by typing error or there is special meaning for this "-1".
Considerd the count of all the invalid data is only 7357, which is only 0.25% of total training data.Hence, I decided to delete those invalid data.Now left 2928492 rows of data.

2.3.1 Check data quality - duplicates data in supplemental datasets
```{r duplicates}
shops[duplicated(shops$shop_name),]
items[duplicated(items$item_name),]
item_categories[duplicated(item_categories$item_category_name),]
```
There are no duplicates data in "shops", "items", and "item_categories" tables which means that for each shop_id, item_id, item_categories_id, they are referring to different shop. item and item' categories.

2.3.2 Check data quality - duplicates data in training dataset
```{r duplicates in train}
train[duplicated(train),]
train <- train[!duplicated(train),]
```
However, there are 6 duplicated rows in training dataset. Therefore I deleted these duplicated rows.

3. Data Preprocessing
3.1 Create dates column
```{r train dates}
train$date <- dmy(train$date)
train$year <- year(train$date)
train$month <- month(train$date)

test$year <- 2015
test$month <- 11
```

3.2 Merge training and testing datasets
```{r merge all}
sub_train <- train[,c("date_block_num","shop_id","item_id","item_category_id", "year", "month","item_price","item_cnt_day")]

test$item_cnt_day <- 0
test$item_price <- 0
sub_test <- test[,c("date_block_num","shop_id","item_id","item_category_id", "year", "month","item_price","item_cnt_day")]

matrix <- rbind(sub_train,sub_test)
```

4. Feature engineering 
4.1 itsm_price and sales count sumarized by shop_id / item_id
```{r feature engineering 1}
matrix_price <- matrix %>% group_by(shop_id,item_id) %>% summarise(price_mean = mean(item_price),cnt_sum = sum(item_cnt_day)) %>% ungroup()

matrix <- matrix %>% left_join(matrix_price, by = c("shop_id", "item_id"))
```

4.2 itsm_price and sales count sumarized by shop_id 
```{r feature engineering 2}
matrix_price1 <- matrix %>% group_by(shop_id) %>% summarise(price_mean_shop = mean(item_price), cnt_sum_shop = sum(item_cnt_day)) %>% ungroup()

matrix <- matrix %>% left_join(matrix_price1, by = c("shop_id"))
```

4.3 itsm_price and sales count sumarized by item_id
```{r feature engineering 3}
matrix_price2 <- matrix %>% group_by(item_id) %>% summarise(price_mean_item = mean(item_price), cnt_sum_item = sum(item_cnt_day)) %>% ungroup()

matrix <- matrix %>% left_join(matrix_price2, by = c("item_id"))
```

4.4 total sale count per month - Prediction Item
```{r create columns:sale per month}
matrix_month <- matrix %>% group_by(year, month, shop_id, item_id) %>% summarise(item_cnt_month = sum(item_cnt_day)) %>% ungroup()

matrix <- matrix %>% left_join(matrix_month, by = c("year","month","shop_id","item_id"))
```


5. Create validation dataset
```{r validation}
train_train <- filter(matrix, date_block_num < 33)
validate <- filter(matrix, date_block_num == 33)
```
We cannot use random sampling the split training and validating datasets here because this is a time series problem, there might be sensonality impact. 
So, I choose last month in training dataset as my validating dataset.

6. Modeling
6.0 Naive Model
```{r Naive}
lm.fit.naive <- lm(item_cnt_month ~ shop_id + item_id, data = train_train)
summary(lm.fit.naive)
lm.pred.naive <-  predict(lm.fit.naive, newdata = validate)
lm.RMSE.naive <- sqrt(mean((lm.pred.naive - validate$item_cnt_month)^2))
lm.RMSE.naive
```
lm.RMSE.naive = 48.184

6.1 Linear regression
```{r Linear regression}
lm.fit <- lm(item_cnt_month ~ shop_id + item_id + item_category_id + cnt_sum + cnt_sum_item + cnt_sum_shop + item_price + price_mean, data = train_train)
summary(lm.fit)
lm.pred <-  predict(lm.fit, newdata = validate)
lm.RMSE <- sqrt(mean((lm.pred - validate$item_cnt_month)^2))
lm.RMSE
```
lm.RMSE = 40.821

6.2 PCR
```{r PCR}
pcr.fit <- pcr(item_cnt_month ~ ., data = train_train, validation = 'CV')
summary(pcr.fit)
validationplot(pcr.fit,val.type = 'MSEP')
pcr.pred <- predict(pcr.fit, newdata = validate, ncomp = 5)
pcr.RMSE <- sqrt(mean((pcr.pred - validate$item_cnt_month)^2))
pcr.RMSE
```
According to the plot, 5 is the best component.
When ncomp = 5, RMSE for PCR model is 40.845 which is comparable with with linear regression model

6.3 PLS
```{r PLS}
pls.fit <- plsr(item_cnt_month ~ ., data = train_train, validation = 'CV')
summary(pls.fit)
validationplot(pls.fit,val.type = 'MSEP')
pls.pred <- predict(pls.fit, newdata = validate, ncomp = 5)
pls.RMSE <- sqrt(mean((pls.pred - validate$item_cnt_month)^2))
pls.RMSE
```
According to the plot, 5 is the best component.
When ncomp = 5, RMSE for PLS model is 40.844 which is comparable with with linear regression model and PCR model.

6.4 Lasso
```{r Lasso}
x_train <-  model.matrix(item_cnt_month~.,train_train)[,-15]
y_train <-  train_train$item_cnt_month
x_validate <- model.matrix(item_cnt_month~.,validate)[,-15]

grid <- 10^seq(10, -2, length=100)
lasso.mod <- glmnet(x_train, y_train, alpha = 1,lambda = grid)
cv.out <- cv.glmnet(x_train, y_train, alpha = 1)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x_validate)
lasso.RMSE <- sqrt(mean((lasso.pred - validate$item_cnt_month)^2))
lasso.RMSE
```
Lasso.RMSE = 44.965

6.5 Polynomial Regression 
```{r Polynomial Regression}
poly.fit1 <- lm(item_cnt_month ~ shop_id + item_id + item_category_id + cnt_sum + I(cnt_sum^2),data = train_train)
poly.fit2 <- lm(item_cnt_month ~ shop_id + item_id + item_category_id + cnt_sum + I(cnt_sum^2) + I(cnt_sum^3),data = train_train)
poly.fit3 <- lm(item_cnt_month ~ poly(cnt_sum,2), data = train_train)
poly.fit4 <- lm(item_cnt_month ~ poly(cnt_sum,3), data = train_train)

poly.pred1 <-  predict(poly.fit1, newdata = validate)
poly.RMSE1 <- sqrt(mean((poly.pred1 - validate$item_cnt_month)^2))
poly.RMSE1

poly.pred2 <-  predict(poly.fit2, newdata = validate)
poly.RMSE2 <- sqrt(mean((poly.pred2 - validate$item_cnt_month)^2))
poly.RMSE2

poly.pred3 <-  predict(poly.fit3, newdata = validate)
poly.RMSE3 <- sqrt(mean((poly.pred3 - validate$item_cnt_month)^2))
poly.RMSE3

poly.pred4 <-  predict(poly.fit4, newdata = validate)
poly.RMSE4 <- sqrt(mean((poly.pred4 - validate$item_cnt_month)^2))
poly.RMSE4
```
The RMSE for the first poly model is 40.428, for the second poly model is 40.567, for the third poly model is 40.490, for the last poly model is 40.627.
Based on this, I think adding polynomial features is an effective way to improve model performance.

6.7 Step Function
```{r Step Function}
step.fit <- lm(item_cnt_month ~ cut(cnt_sum,4), data = train_train)
step.pred <-  predict(step.fit, newdata = validate)
step.RMSE <- sqrt(mean((step.pred - validate$item_cnt_month)^2))
step.RMSE
```
Step.RMSE = 41.091

6.6 Spline Line
```{r Spline line}
cntlims=range(matrix$cnt_sum)
cnt.grid=seq(from=cntlims[1],to=cntlims[2])
sp.fit=lm(item_cnt_month ~  bs(cnt_sum,knots=c(6,16,48)),data=train_train)
sp.pred=predict(sp.fit,newdata = validate,se=T)
sp.RMSE <- sqrt(mean((sp.pred$fit - validate$item_cnt_month)^2))
sp.RMSE
```
sp.RMSE = 40.604

6.7 Natural Spline Line
```{r Natural Spline Line }
nsp.fit <- lm(item_cnt_month ~  ns(cnt_sum,df=4),data=train_train)
nsp.pred <- predict(nsp.fit,newdata = validate,se=T)
nsp.RMSE <- sqrt(mean((nsp.pred$fit - validate$item_cnt_month)^2))
nsp.RMSE
```
nsp.RMSE = 40.695

6.8 XG boost
```{r XG boost}
trainMatrix <- sparse.model.matrix(item_cnt_month ~ shop_id + item_id + item_category_id + cnt_sum + cnt_sum_item + cnt_sum_shop + item_price + price_mean,
                                   data = train_train,
                                   contrasts.arg = c('shop_id', 'item_id'),
                                   sparse = TRUE, sci = FALSE)
#Create input for xgboost
trainDMatrix <- xgb.DMatrix(data = trainMatrix, label = train_train$item_cnt_month)

testMatrix <- sparse.model.matrix(item_cnt_month ~ shop_id + item_id + item_category_id + cnt_sum + cnt_sum_item + cnt_sum_shop + item_price + price_mean,
                                  data = validate,
                                  contrasts.arg = c('shop_id', 'item_id'),
                                  sparse = TRUE, sci = FALSE)
#Create input for xgboost
testDMatrix <- xgb.DMatrix(data = testMatrix, label = validate$item_cnt_month)

params <- list(booster = "gbtree",
               objective = "reg:linear",
               eval_metric = "rmse",
               eta = 0.5,       # no need more complexity
               min_child_weight = 5,
               #colsample_bytree = 1,
               gamma = 0.9,
               alpha = 1.0,
               max_depth = 10,    # no nead more depth
               subsample = 1,
               print_every_n = 20
               )

# parallel calculation
N_cpu = parallel::detectCores()

#Cross-validation
xgb.tab <- xgb.cv(data = trainDMatrix,
                  params = params,
                  maximize = FALSE, nrounds = 1000,
                  nthreads = N_cpu, nfold = 5, early_stopping_round = 10)

#Best Number of iterations
num_iterations = xgb.tab$best_iteration

# Train the model with optimized n_round
xgboost_tree <- xgb.train(data = trainDMatrix
                               , param = params
                               , maximize = TRUE, evaluation = 'rmse', nrounds = num_iterations)

# Visualize features importances
importance <- xgb.importance(feature_names = colnames(trainMatrix), model = xgboost_tree)
library(Ckmeans.1d.dp)
xgb.ggplot.importance(importance_matrix = importance)

# Prediction
pred_tree <- predict(xgboost_tree, testDMatrix)
XGboost.RMSE <- sqrt(mean((pred_tree - validate$item_cnt_month)^2))
XGboost.RMSE
```
XGboost.RMSE = 42.02

7. Final Model
Based all modeling discussion above, Linear Regression, PCR, PLS and Spline lines have low RMSE. Choose PCR as my final model.
Consiered the feature importance ranking from XGboost, only keep cnt_sum, item_price, item_category_id and cnt_sum_shop in the final model.

Considered there are more than 10 features, use PCR model to select features rather than manual selection. The number of principal components, to incorporate in the model, is chosen by cross-validation (cv).Also, PCR is suitable when the data set contains highly correlated predictors as our model.
```{r final model}
train_final <- filter(matrix, date_block_num != 34)
test_final <- filter(matrix, date_block_num == 34)

final.fit <- pcr(item_cnt_month ~ cnt_sum + item_price + item_category_id + cnt_sum_shop, data = train_final, validation = 'CV')

validationplot(final.fit,val.type = 'MSEP')
final.pred <- predict(final.fit, newdata = test_final, ncomp = 3)

```